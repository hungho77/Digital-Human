# Digital Human - Pipecat-Inspired Architecture Proposal

## Overview

This proposal reorganizes the Digital Human codebase following Pipecat's frame-based pipeline architecture. The key changes:

1. **Rename "Real" â†’ "Avatar"** - More understandable name for the talking head video generator
2. **Frame-based data flow** - All data (audio, video, text) flows as typed frames
3. **Processor composition** - Small, focused processors instead of monolithic classes
4. **Clean separation** - Transports, Processors, Services, and Pipelines are distinct

---

## ğŸ¯ Core Concepts

### Frame Types
Frames are the atomic units of data flowing through the system:

```python
# src/core/frames.py
from dataclasses import dataclass
from typing import Optional
import numpy as np

@dataclass
class Frame:
    """Base frame class"""
    timestamp: Optional[float] = None

@dataclass
class TextFrame(Frame):
    """Text data (user input, LLM output, transcription)"""
    text: str
    user_id: Optional[str] = None

@dataclass
class AudioRawFrame(Frame):
    """Raw audio data (PCM16, 16kHz mono)"""
    audio: np.ndarray  # shape: (samples,)
    sample_rate: int = 16000
    num_channels: int = 1

@dataclass
class VideoFrame(Frame):
    """Video frame data"""
    image: np.ndarray  # shape: (H, W, 3), BGR format
    width: int
    height: int

@dataclass
class AvatarFrame(Frame):
    """Generated avatar video frame with corresponding audio"""
    video: np.ndarray  # Talking head frame
    audio: np.ndarray  # Synced audio chunk
    is_speaking: bool

@dataclass
class SystemFrame(Frame):
    """Control signals"""
    pass

@dataclass
class StartFrame(SystemFrame):
    """Pipeline start signal"""
    pass

@dataclass
class EndFrame(SystemFrame):
    """Pipeline end signal"""
    pass

@dataclass
class CancelFrame(SystemFrame):
    """Cancel current operation"""
    pass
```

### Processor Base Class

```python
# src/core/processor.py
from abc import ABC, abstractmethod
from asyncio import Queue
from typing import Optional
from src.core.frames import Frame, SystemFrame

class FrameProcessor(ABC):
    """Base processor following Pipecat pattern"""

    def __init__(self):
        self._prev: Optional[FrameProcessor] = None
        self._next: Optional[FrameProcessor] = None
        self._input_queue: Queue = Queue()
        self._system_queue: Queue = Queue()  # Priority queue for system frames

    def link(self, next_processor: 'FrameProcessor') -> 'FrameProcessor':
        """Link this processor to the next one"""
        self._next = next_processor
        next_processor._prev = self
        return next_processor

    async def queue_frame(self, frame: Frame):
        """Queue a frame for processing"""
        if isinstance(frame, SystemFrame):
            await self._system_queue.put(frame)
        else:
            await self._input_queue.put(frame)

    async def push_frame(self, frame: Frame):
        """Push frame to next processor"""
        if self._next:
            await self._next.queue_frame(frame)

    async def push_error(self, error: Exception):
        """Push error upstream"""
        # Create ErrorFrame and push to previous processor
        pass

    @abstractmethod
    async def process_frame(self, frame: Frame):
        """Process a single frame (override in subclasses)"""
        pass

    async def process(self):
        """Main processing loop"""
        while True:
            # Process system frames first (priority)
            if not self._system_queue.empty():
                frame = await self._system_queue.get()
                await self.process_frame(frame)
            else:
                frame = await self._input_queue.get()
                await self.process_frame(frame)
```

### Pipeline

```python
# src/core/pipeline.py
from typing import List
from src.core.processor import FrameProcessor
from src.core.frames import StartFrame, EndFrame

class Pipeline:
    """Orchestrates processor chains (Pipecat-style)"""

    def __init__(self, processors: List[FrameProcessor]):
        self.processors = processors
        self._link_processors()

    def _link_processors(self):
        """Chain processors together"""
        for i in range(len(self.processors) - 1):
            self.processors[i].link(self.processors[i + 1])

    async def start(self):
        """Start all processors"""
        # Start processing tasks for each processor
        tasks = [asyncio.create_task(p.process()) for p in self.processors]

        # Send start frame
        await self.processors[0].queue_frame(StartFrame())

        await asyncio.gather(*tasks)

    async def stop(self):
        """Stop pipeline"""
        await self.processors[0].queue_frame(EndFrame())
```

---

## ğŸ“ Proposed Directory Structure

```
src/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ app.py                          # Main entry point
â”‚
â”œâ”€â”€ core/                           # Core framework (Pipecat-inspired)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ frames.py                   # Frame data structures â­ NEW
â”‚   â”œâ”€â”€ processor.py                # Base FrameProcessor â­ NEW
â”‚   â”œâ”€â”€ pipeline.py                 # Pipeline orchestration â­ NEW
â”‚   â”œâ”€â”€ context.py                  # Shared context/state â­ NEW
â”‚   â””â”€â”€ exceptions.py               # Custom exceptions â­ NEW
â”‚
â”œâ”€â”€ processors/                     # Frame processors (Pipecat-style)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ avatar/                     # Avatar generation (renamed from "real") â­ RENAMED
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_avatar.py          # Base avatar processor interface
â”‚   â”‚   â”œâ”€â”€ avatar_processor.py     # Main avatar frame processor
â”‚   â”‚   â”œâ”€â”€ lip_sync_processor.py   # Lip-sync specific processor
â”‚   â”‚   â””â”€â”€ frame_composer.py       # Compose video + audio frames
â”‚   â”‚
â”‚   â”œâ”€â”€ audio/                      # Audio processing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ audio_buffer.py         # Audio buffering/chunking
â”‚   â”‚   â”œâ”€â”€ resampler.py            # Audio resampling
â”‚   â”‚   â””â”€â”€ vad_processor.py        # Voice activity detection
â”‚   â”‚
â”‚   â”œâ”€â”€ video/                      # Video processing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ frame_renderer.py       # Video frame rendering
â”‚   â”‚   â””â”€â”€ transition_processor.py # Fade transitions
â”‚   â”‚
â”‚   â””â”€â”€ text/                       # Text processing
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ text_processor.py       # Text preprocessing
â”‚
â”œâ”€â”€ transports/                     # I/O boundaries (Pipecat-style)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_transport.py           # Base transport interface
â”‚   â”‚
â”‚   â”œâ”€â”€ webrtc/                     # WebRTC transport
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ webrtc_transport.py     # WebRTC processor
â”‚   â”‚   â”œâ”€â”€ player.py               # Media player (from services/webrtc.py)
â”‚   â”‚   â””â”€â”€ tracks.py               # Audio/Video track handlers
â”‚   â”‚
â”‚   â”œâ”€â”€ virtualcam/                 # Virtual camera transport
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ virtualcam_transport.py # pyvirtualcam integration
â”‚   â”‚
â”‚   â””â”€â”€ local/                      # Local file I/O (testing)
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ local_transport.py      # File input/output
â”‚
â”œâ”€â”€ services/                       # AI service integrations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ tts/                        # Text-to-Speech
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_tts.py             # Base TTS interface
â”‚   â”‚   â”œâ”€â”€ tts_processor.py        # TTS frame processor â­ NEW
â”‚   â”‚   â”œâ”€â”€ edgetts.py              # EdgeTTS implementation
â”‚   â”‚   â”œâ”€â”€ xtts.py                 # XTTS implementation
â”‚   â”‚   â”œâ”€â”€ cosyvoice.py            # CosyVoice implementation
â”‚   â”‚   â”œâ”€â”€ gpt_sovits.py           # GPT-SoVITS implementation
â”‚   â”‚   â””â”€â”€ fishtts.py              # FishTTS implementation
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/                        # Large Language Models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_llm.py             # Base LLM interface
â”‚   â”‚   â”œâ”€â”€ llm_processor.py        # LLM frame processor â­ NEW
â”‚   â”‚   â”œâ”€â”€ openai_llm.py           # OpenAI (GPT-4, etc.)
â”‚   â”‚   â”œâ”€â”€ anthropic_llm.py        # Anthropic (Claude)
â”‚   â”‚   â”œâ”€â”€ google_llm.py           # Google (Gemini)
â”‚   â”‚   â””â”€â”€ local_llm.py            # vLLM + Qwen3
â”‚   â”‚
â”‚   â”œâ”€â”€ asr/                        # Automatic Speech Recognition
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_asr.py             # Base ASR interface
â”‚   â”‚   â”œâ”€â”€ asr_processor.py        # ASR frame processor â­ NEW
â”‚   â”‚   â”œâ”€â”€ whisper_asr.py          # Whisper ASR
â”‚   â”‚   â””â”€â”€ silero_vad.py           # Silero VAD
â”‚   â”‚
â”‚   â””â”€â”€ avatar_models/              # Avatar model implementations (renamed) â­ RENAMED
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ model_manager.py        # Model loading/caching (from services/real.py)
â”‚       â”œâ”€â”€ musetalk/               # MuseTalk
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ musetalk_avatar.py
â”‚       â”œâ”€â”€ wav2lip/                # Wav2Lip
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ wav2lip_avatar.py
â”‚       â”œâ”€â”€ ultralight/             # Ultralight
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ ultralight_avatar.py
â”‚       â””â”€â”€ synctalk/               # SyncTalk (future)
â”‚           â”œâ”€â”€ __init__.py
â”‚           â””â”€â”€ synctalk_avatar.py
â”‚
â”œâ”€â”€ agents/                         # LangGraph conversational agents
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_agent.py               # Base agent interface
â”‚   â”œâ”€â”€ orchestrator.py             # Main orchestrator
â”‚   â”œâ”€â”€ dialogue_agent.py           # Dialogue handling
â”‚   â””â”€â”€ reservation_agent.py        # Reservation tasks
â”‚
â”œâ”€â”€ rag/                           # RAG system (Retrieval-Augmented Generation)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ vectorstore.py              # Qdrant/vector DB
â”‚   â”œâ”€â”€ retriever.py                # Knowledge retrieval
â”‚   â”œâ”€â”€ embeddings.py               # Embedding generation
â”‚   â””â”€â”€ document_loader.py          # Document loading
â”‚
â”œâ”€â”€ api/                           # FastAPI server
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ server.py                   # FastAPI app
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ webrtc.py               # WebRTC endpoints
â”‚   â”‚   â”œâ”€â”€ chat.py                 # Chat endpoints
â”‚   â”‚   â””â”€â”€ session.py              # Session management
â”‚   â””â”€â”€ middleware/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ cors.py                 # CORS
â”‚       â””â”€â”€ error_handler.py        # Error handling
â”‚
â”œâ”€â”€ config/                        # Configuration
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py                 # Pydantic settings
â”‚   â””â”€â”€ config_examples.json
â”‚
â””â”€â”€ utils/                         # Utilities
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ logger.py
    â””â”€â”€ helpers.py
```

---

## ğŸ”„ Example Pipeline Construction

### Scenario: User speaks â†’ Avatar responds

```python
# Example: Building a conversational avatar pipeline

from src.core.pipeline import Pipeline
from src.transports.webrtc import WebRTCTransport
from src.processors.audio import VADProcessor, AudioBuffer
from src.services.asr import WhisperASRProcessor
from src.services.llm import OpenAILLMProcessor
from src.services.tts import EdgeTTSProcessor
from src.processors.avatar import AvatarProcessor
from src.services.avatar_models import MuseTalkAvatar

# Create processors
webrtc = WebRTCTransport()           # Input: WebRTC audio/video
vad = VADProcessor()                  # Detect speech
asr = WhisperASRProcessor()           # Audio â†’ Text
llm = OpenAILLMProcessor()            # Text â†’ Response
tts = EdgeTTSProcessor()              # Text â†’ Audio
avatar = AvatarProcessor(
    model=MuseTalkAvatar()            # Audio â†’ Talking head video
)

# Build pipeline
pipeline = Pipeline([
    webrtc,      # Receive audio from user
    vad,         # Detect voice activity
    asr,         # Transcribe speech
    llm,         # Generate response
    tts,         # Synthesize speech
    avatar,      # Generate talking head
    webrtc       # Send video/audio back
])

# Start
await pipeline.start()
```

### Data Flow:

```
User Speech (WebRTC)
  â†“
AudioRawFrame
  â†“
VADProcessor â†’ AudioRawFrame (with VAD info)
  â†“
WhisperASRProcessor â†’ TextFrame("Hello, how are you?")
  â†“
OpenAILLMProcessor â†’ TextFrame("I'm doing well, thank you!")
  â†“
EdgeTTSProcessor â†’ AudioRawFrame (synthesized speech)
  â†“
AvatarProcessor â†’ AvatarFrame (video + audio)
  â†“
WebRTCTransport â†’ Send to user
```

---

## ğŸ¨ Key Architectural Improvements

### 1. **Small, Focused Processors**
Instead of monolithic `BaseReal`:
- `AvatarProcessor` - Generates talking head frames
- `LipSyncProcessor` - Handles lip synchronization
- `TransitionProcessor` - Smooth transitions between states
- `FrameComposer` - Combines video + audio

### 2. **Service Processors**
Wrap AI services as processors:
- `TTSProcessor(BaseTTS)` - Text â†’ Audio frames
- `LLMProcessor(BaseLLM)` - Text â†’ Text frames
- `ASRProcessor(BaseASR)` - Audio â†’ Text frames

### 3. **Transport Abstraction**
- `WebRTCTransport` - Handles WebRTC I/O
- `VirtualCamTransport` - Outputs to virtual camera
- `LocalTransport` - File-based testing

### 4. **Frame-Based Communication**
Everything flows as typed frames:
- Type safety
- Easy debugging
- Clear data flow
- Composable pipelines

### 5. **Separation of Concerns**

| Layer | Responsibility | Examples |
|-------|----------------|----------|
| **Frames** | Data structures | `AudioFrame`, `VideoFrame`, `TextFrame` |
| **Processors** | Transform frames | `VADProcessor`, `AvatarProcessor` |
| **Services** | AI integrations | `EdgeTTS`, `OpenAI`, `MuseTalk` |
| **Transports** | I/O boundaries | `WebRTC`, `VirtualCam` |
| **Pipeline** | Orchestration | Link processors, manage flow |

---

## ğŸš€ Migration Strategy

### Phase 1: Core Framework (Week 1-2)
âœ… Create frame types (`frames.py`)
âœ… Implement `FrameProcessor` base class
âœ… Build `Pipeline` orchestrator
âœ… Add basic frame flow tests

### Phase 2: Refactor Avatar (Week 3-4)
âœ… Rename `BaseReal` â†’ `BaseAvatar`
âœ… Extract `AvatarProcessor` from monolithic class
âœ… Move video rendering to `FrameRenderer`
âœ… Create `LipSyncProcessor`
âœ… Update model loading in `services/avatar_models/`

### Phase 3: Service Processors (Week 5-6)
âœ… Create `TTSProcessor` wrapper
âœ… Create `LLMProcessor` wrapper
âœ… Create `ASRProcessor` wrapper
âœ… Ensure all services output frames

### Phase 4: Transport Layer (Week 7-8)
âœ… Refactor WebRTC as `WebRTCTransport` processor
âœ… Create `VirtualCamTransport`
âœ… Move `HumanPlayer` to `transports/webrtc/player.py`

### Phase 5: Integration & Testing (Week 9-10)
âœ… Build example pipelines
âœ… Integration tests
âœ… Performance benchmarking
âœ… Documentation

---

## ğŸ¯ Benefits

### Developer Experience
- **Easier to understand**: Small, focused components
- **Easier to test**: Each processor can be unit tested
- **Easier to extend**: Add new processors without touching existing code
- **Easier to debug**: Frame flow is explicit and traceable

### Performance
- **Async-first**: Native async/await throughout
- **Parallel processing**: Processors can run concurrently
- **Backpressure**: Queue-based flow control

### Flexibility
- **Swap components**: Change TTS/LLM without pipeline changes
- **Multiple transports**: WebRTC, VirtualCam, or file-based
- **Custom pipelines**: Compose processors for different use cases

---

## ğŸ“š Pipecat Alignment

This architecture directly mirrors Pipecat's design:

| Pipecat Concept | Our Implementation |
|-----------------|-------------------|
| `Frame` | `src/core/frames.py` |
| `FrameProcessor` | `src/core/processor.py` |
| `Pipeline` | `src/core/pipeline.py` |
| `Transport` | `src/transports/` |
| Service integrations | `src/services/` |
| Processor composition | Chain via `.link()` |

---

## â“ FAQ

**Q: Why rename "Real" to "Avatar"?**
A: "Real" is ambiguous. "Avatar" clearly indicates it's generating a talking head/digital human.

**Q: Is this a complete rewrite?**
A: No. We're refactoring existing code into smaller, composable pieces. Core logic (MuseTalk, Wav2Lip) stays the same.

**Q: Will existing APIs break?**
A: We'll maintain backward compatibility during migration. Old endpoints can internally use the new pipeline.

**Q: What about performance?**
A: Frame-based architecture with async queues typically improves performance through better concurrency.

---

## ğŸ¬ Next Steps

1. **Review & Approve** this proposal
2. **Create feature branch** for refactoring
3. **Implement Phase 1** (core framework)
4. **Test with simple pipeline** (e.g., TTS â†’ Avatar)
5. **Gradually migrate** existing functionality
6. **Update documentation** with new architecture

---

**Questions? Feedback?** Let's discuss! ğŸš€
